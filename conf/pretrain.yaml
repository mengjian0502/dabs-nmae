algorithm: mae
data_root: /share/seo/mscoco/
gpus: 0
budget: 0.05
corruption_rate: 0.85

framework: pytorch

exp:
  base_dir: /home/jm2787/dabs-nmae/
  name: mae-mscoco-sparsity0.85

trainer:
  weights_summary: top
  seed: 0
  val_check_interval: 1.0
  limit_val_batches: 1.0
  resume_from_checkpoint: null
  precision: 16  # set to 16 for O1 mixed precision, 32 for O0 full precision
  max_steps: 100_000
  ckpt_every_n_steps: 20_000
  gradient_clip_val: 0
  strategy: ddp
  accumulate_grad_batches: 1

optim:
  name: adam
  lr: 0.0001
  weight_decay: 0.0001
  momentum: 0.9  # only used for momentum-based optimizers

contpred:
  normalize: 0
  symmetric_loss: 0

defaults:
  - model: transformer
  - dataset: mscoco

# Disable hydra creation of directories
hydra:
  output_subdir: null
  run:
      dir: .
